{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing Assignment 1: word2vec\n",
    "\n",
    "### Evan James Heetderks / 高修杰 / 2019280025\n",
    "\n",
    "##### Disclaimer: Some source code used from https://github.com/Adoni/word2vec_pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART I: Gradients Calculation\n",
    "\n",
    "Please refer to \"2019280025_Word2Vec\\Report\\Gradients_Calculation.pdf\" to see my gradient derivations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART II: Word2vec Implementation\n",
    "\n",
    "I first downloaded a 100 megabyte wikipedia dump file \"enwiki-latest-pages-articles14.xml-p7697595p7744800.bz2\" from the following website: https://dumps.wikimedia.org/enwiki/latest/. I have not included it in my deliverables folder due to it's size.\n",
    "\n",
    "I then implemented word2vec by combining existing code (see disclaimer above) with my own methods to generate a wikipedia corpus that is free of selected stopwords and other words that appear under a minimum amount. We import the top-level source file here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a model with 100 embedding dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now initialize the first model (100 embedding dimensions) using the wikipedia dump file above. The other parameters will be the same for each model in part II."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus is ready...\n",
      "Word Count: 41283\n",
      "Sentence Length: 2172215\n"
     ]
    }
   ],
   "source": [
    "Model_1 = Word2Vec(wikidump_filename=\"enwiki-latest-pages-articles14.xml-p7697595p7744800.bz2\",\n",
    "               output_text_filename=\"corpus1.txt\", \n",
    "               emb_dimension=100, \n",
    "               batch_size=50,\n",
    "               window_size= 8, \n",
    "               iteration=1, \n",
    "               initial_lr=0.025, \n",
    "               min_count=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now train the 1st model (100 embedding dimensions) here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 133.50167847, lr: 0.000063: 100%|█████████████████████████████████████| 645635/645635 [1:11:14<00:00, 151.04it/s]\n"
     ]
    }
   ],
   "source": [
    "Model_1.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now calculate the spearman coefficient of the 1st model (100 embedding dimensions) here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman coefficient for 100 dimensions:  0.2806071138329367\n"
     ]
    }
   ],
   "source": [
    "spearman_coefficient = Model_1.wordsim353_spearman(\"combined.csv\")\n",
    "print('Spearman coefficient for 100 dimensions: ', spearman_coefficient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a model with 200 embedding dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now initialize the second model (200 embedding dimensions) using the wikipedia dump file at the beginning of this document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus is ready...\n",
      "Word Count: 41283\n",
      "Sentence Length: 2172215\n"
     ]
    }
   ],
   "source": [
    "Model_2 = Word2Vec(wikidump_filename=\"enwiki-latest-pages-articles14.xml-p7697595p7744800.bz2\",\n",
    "               output_text_filename=\"corpus2.txt\", \n",
    "               emb_dimension=200, \n",
    "               batch_size=50,\n",
    "               window_size= 8, \n",
    "               iteration=1, \n",
    "               initial_lr=0.025, \n",
    "               min_count=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now train the 1st model (200 embedding dimensions) here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 131.42134094, lr: 0.000063: 100%|█████████████████████████████████████| 645635/645635 [1:03:23<00:00, 169.73it/s]\n"
     ]
    }
   ],
   "source": [
    "Model_2.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now calculate the spearman coefficient of the 2nd model (200 embedding dimensions) here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman coefficient for 200 dimensions:  0.2810809339989552\n"
     ]
    }
   ],
   "source": [
    "spearman_coefficient = Model_2.wordsim353_spearman(\"combined.csv\")\n",
    "print('Spearman coefficient for 200 dimensions: ', spearman_coefficient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a model with 300 embedding dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now initialize the third model (300 embedding dimensions) using the wikipedia dump file at the beginning of this document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus is ready...\n",
      "Word Count: 41283\n",
      "Sentence Length: 2172215\n"
     ]
    }
   ],
   "source": [
    "Model_3 = Word2Vec(wikidump_filename=\"enwiki-latest-pages-articles14.xml-p7697595p7744800.bz2\",\n",
    "               output_text_filename=\"corpus3.txt\", \n",
    "               emb_dimension=300, \n",
    "               batch_size=50,\n",
    "               window_size= 8, \n",
    "               iteration=1, \n",
    "               initial_lr=0.025, \n",
    "               min_count=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now train the 1st model (300 embedding dimensions) here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 136.37429810, lr: 0.000063: 100%|███████████████████████████████████████| 645635/645635 [59:26<00:00, 181.01it/s]\n"
     ]
    }
   ],
   "source": [
    "Model_3.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now calculate the spearman coefficient of the 3rd model (300 embedding dimensions) here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman coefficient for 300 dimensions:  0.28626610843443434\n"
     ]
    }
   ],
   "source": [
    "spearman_coefficient = Model_3.wordsim353_spearman(\"combined.csv\")\n",
    "print('Spearman coefficient for 300 dimensions: ', spearman_coefficient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "#### Spearman coefficient (100 dimensions): 0.2806071138329367\n",
    "#### Spearman coefficient (200 dimensions): 0.2810809339989552\n",
    "#### Spearman coefficient (300 dimensions): 0.28626610843443434\n",
    "\n",
    "#### We thus confirm that the model performs best with a higher number of embedding dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART III: Word2vec Improvement\n",
    "\n",
    "To further improve the model, the window size was increased from 8 to 10, which will allow the target word to map to more context words and thus increase the reliability of the model. The model retains the same 300 embedding dimension size as in Part II."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus is ready...\n",
      "Word Count: 41283\n",
      "Sentence Length: 2172215\n"
     ]
    }
   ],
   "source": [
    "Model_4 = Word2Vec(wikidump_filename=\"enwiki-latest-pages-articles14.xml-p7697595p7744800.bz2\",\n",
    "               output_text_filename=\"corpus4.txt\", \n",
    "               emb_dimension=300, \n",
    "               batch_size=50,\n",
    "               window_size= 10, \n",
    "               iteration=1, \n",
    "               initial_lr=0.025, \n",
    "               min_count=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now train the improved model (300 embedding dimensions, window size 10) here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 116.94097900, lr: 0.000007: 100%|█████████████████████████████████████| 816230/816230 [1:13:32<00:00, 184.99it/s]\n"
     ]
    }
   ],
   "source": [
    "Model_4.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now calculate the spearman coefficient of the improved model (300 embedding dimensions, window size 10) here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman coefficient for 300 dimensions, window size 10:  0.31037887473152614\n"
     ]
    }
   ],
   "source": [
    "spearman_coefficient = Model_4.wordsim353_spearman(\"combined.csv\")\n",
    "print('Spearman coefficient for 300 dimensions, window size 10: ', spearman_coefficient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "#### Spearman coefficient (300 dimensions, window size 10): 0.31037887473152614\n",
    "\n",
    "#### We thus confirm that the model performs best with a higher window size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
